# -*- coding: utf-8 -*-
"""cifar10_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1abB4ZYeYqbYHpkXDLizliuKKmiNdi5ei
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader
from multiprocessing import get_context

# Hyperparameters
learning_rate = 0.005
momentum = 0.8 # For SGD optimizer
weight_decay = 0.0005
batch_size = 25
num_blocks = 7 # Number of blocks in the backbone
num_epochs = 25
num_conv = 3 # Number of conv layers per block
dropout = 0.1 # Dropout value for conv layers in Block
T_max = num_epochs # For Cosine Annealing
eta_min = 0.00001 # For Cosine Annealing

# Transforming the incoming data to tensor, normalizing, and performing data augmentation
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4)
])

# Task 1
# Downloading/verifying CIFAR-10 dataset and creating dataloaders for both training and testing sets
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, multiprocessing_context=get_context('spawn'))
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2, multiprocessing_context=get_context('spawn'))

# Task 2
class Block(nn.Module):
    def __init__(self, input_channels, output_channels, k):
        super(Block, self).__init__()
        # Calculates the spatial average pooling per channel which returns a vector of d channels
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        # Linear layer for predicting vector a
        self.linear = nn.Linear(input_channels, num_conv)
        # List of K convolutional layers with batch normalization, ReLU activation, and dropout
        self.convs = nn.ModuleList([nn.Sequential(nn.Conv2d(input_channels, output_channels, 3, padding=1), 
                                                  nn.BatchNorm2d(output_channels), nn.ReLU(inplace=True), 
                                                  nn.Dropout(dropout)) for i in range(num_conv)])
        # ResNet-like skip connection which uses a 1x1 conv to make the input and output channels equal as well as batch normalization
        self.skip = nn.Sequential(
              nn.Conv2d(input_channels, output_channels, kernel_size=1, stride=1, padding=0),
              nn.BatchNorm2d(output_channels)
        )

    def forward(self, x):
        # Input stored to be later used for the skip connection
        residual = x
        # Performing spatial average pooling
        x_pooled= self.pool(x)
        # Reshaping x_pooled to prepare it for the linear layer predicting vector a
        x_pooled = torch.flatten(x_pooled, 1)
        # Predicting vector a
        a = self.linear(x_pooled)
        # Applying non-linear activation function g
        a = F.relu(a)
        # a is reshaped to be compatible with conv layers so that they can be combined to produce a single output
        x = sum(a[:, i].view(-1, 1, 1, 1) * self.convs[i](x) for i in range(len(self.convs)))
        # Adding the skip connection to the output of the block
        x += self.skip(residual)
        return x

# Task 2
class Model(nn.Module):
    def __init__(self, num_blocks, input_channels, output_channels, k):
        super(Model, self).__init__()
        # Create a sequence of Block instances (backbone)
        layers = [Block(input_channels, output_channels, num_conv)]
        for i in range(num_blocks - 1):
            layers.append(Block(output_channels, output_channels, num_conv))
        # Stack the blocks sequentially (backbone)
        self.blocks = nn.Sequential(*layers)
        # B. Spatial average pooling layer for the final block's output (classifier)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        # C. Classifier for the final output (softmax regression classifier)
        self.classifier = nn.Linear(output_channels, 10)

    def forward(self, x):
        # Pass the input through the sequence of blocks (backbone)
        x = self.blocks(x)
        # Computes the mean featuring by applying spatial average pooling to the output of the last block (classifier)
        x = self.pool(x)
        # Reshaping x_pooled to prepare it for the classifier
        x = torch.flatten(x, 1)
        # Applying the classifier to obtain the final output
        x = self.classifier(x)
        return x


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)

# Task 3
# Model, loss, optimizer, and scheduler instantiation
model = Model(num_blocks=num_blocks, input_channels=3, output_channels=64, k=num_conv).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)

train_losses = []
train_accuracies = []
test_losses = []
test_accuracies = []
best_acc = 0

# Task 4
# Training function
def train(train_loader, model, criterion, optimizer, device):
    model.train()
    cumulative_loss = 0.0
    correct = 0
    samples = 0
    
    for i, (images,labels) in enumerate(train_loader):
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        cumulative_loss += loss.item()
        predicted = torch.argmax(outputs, dim=1)
        samples += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    epoch_loss = cumulative_loss / len(train_loader)
    epoch_acc = 100. * correct / samples
    return epoch_loss, epoch_acc

# Task 4
# Testing function
def test(test_loader, model, criterion, device):
    model.eval()
    cumulative_loss = 0.0
    correct = 0
    samples = 0
    
    with torch.no_grad():
        for i, (images, labels) in enumerate(test_loader):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            cumulative_loss += loss.item()
            predicted = torch.argmax(outputs, dim=1)
            samples += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    epoch_loss = cumulative_loss / len(test_loader)
    epoch_acc = 100. * correct / samples
    return epoch_loss, epoch_acc

# Task 4
# Training and testing loop
for epoch in range(num_epochs):
    train_loss, train_acc = train(train_loader=trainloader, model=model, criterion=criterion, optimizer=optimizer, device=device)
    test_loss, test_acc = test(test_loader=testloader, model=model, criterion=criterion, device=device)
    scheduler.step()

    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    test_losses.append(test_loss)
    test_accuracies.append(test_acc)
    
    print(f"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%")
    
    if test_acc > best_acc:
        best_acc = test_acc

print(f"Best Test Accuracy: {best_acc:.2f}%")

# Loss evolution graph
plt.figure()
plt.plot(train_losses, label="Training Loss")
plt.plot(test_losses, label="Testing Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Loss Evolution")
plt.show()

# Accuracy evolution graph
plt.figure()
plt.plot(train_accuracies, label="Training Accuracy")
plt.plot(test_accuracies, label="Testing Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy (%)")
plt.legend()
plt.title("Accuracy Evolution")
plt.show()